
<!-- saved from url=(0046)http://www.cs.cmu.edu/~peiyunh/tiny/index.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <link rel="StyleSheet" href="./files/project.css" type="text/css" media="all">
  <link href="./files/css" rel="stylesheet">
  <title>Just Go with the Flow: Self-Supervised Scene Flow Estimation</title>
</head>

<body>

  <div class="content content-title" style="text-align: center">
	    <h1>Just Go with the Flow: Self-Supervised Scene Flow Estimation</h1>
	    <p id="authors">
	      <a href="https://himangim.github.io/" target="_blank">Himangi Mittal</a>
	      <a href="https://www.ri.cmu.edu/ri-people/brian-e-okorn/" target="_blank">Brian Okorn</a>
	      <a href="https://davheld.github.io/" target="_blank">David Held</a>
                <br>
	      Robotics Institute <br>
	      Carnegie Mellon University
	    </p>
	    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Mittal_Just_Go_With_the_Flow_Self-Supervised_Scene_Flow_Estimation_CVPR_2020_paper.pdf">[Paper]</a>
	    <a href="https://arxiv.org/abs/1912.00497">[Arxiv Paper]</a>
	    <a href="https://github.com/HimangiM/Just-Go-with-the-Flow-Self-Supervised-Scene-Flow-Estimation">[Code]</a> 
	    <a href="https://youtu.be/7fTjCwhO140" target="_blank">[1-minute-video]</a>
	    <a href="https://youtu.be/a5HFnDPTNLk" target="_blank">[5-minute-video]</a>
              <!-- <img src="./files/GitHub-Mark.png" alt="code-releasing-soon" width="64px"> -->
            
    </div>
    
  
  
  <div class="content" style=’text-align:center;’>
  <!-- <a href="./files/cvpr-9651.mp4"> -->
        <!-- <video width="720px" style="display:block; margin: 0 auto;" controls> -->
        <!-- <source src="./files/cvpr-9651.mp4" width="100%" alt="teaser" style="text-align:center"> -->
	<iframe width="560" height="315" src="https://www.youtube.com/embed/a5HFnDPTNLk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	<!-- </video> -->
      <!-- </a> -->
  </div>
  
<div class="content">
  <div class="row">
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
     <a href="./files/front_page_image">
        <img src="./files/front_page_image" width="100%" alt="teaser" style="margin-bottom:10px">
	<!-- <div class="column"> -->
	     <!-- <img src="./files/overview_car.png" width="50%" alt="teaser" style="margin-bottom:10px"> -->
	<!-- </div> -->
	<!-- <div class="column"> -->
		<!-- <img src="./files/twitter.gif" width="50%" alt="teaser_gif" style="margin-bottom:10px"> -->
	<!-- </div> -->
      </a>
      <figcaption> (Left) We use two self-supervised losses to learn scene flow on large unlabeled datasets. The "nearest neighbor loss" penalizes the distance between the predicted point cloud <font color="green">(green)</font> and each predicted point's nearest neighbor in the second point cloud <font color="red">(red)</font>.  To avoid degenerate solutions, we estimate the flow between these predicted points <font color="green">(green)</font> in the reverse direction back to the original point cloud <font color="blue">(blue)</font> to form a cycle. The new predicted points from the cycle <font color="magenta">(purple)</font> should align with the original points <font color="blue">(blue)</font> and the distance between these two set of points, forms our second self-supervised loss: "cycle consistency". (Right) For the NuScenes dataset, scene flow is computed between point cloud at time 't' <font color="red">(red)</font> and 't+1' <font color = "green">(green)</font> and the transformed cloud in shown in <font color="blue">(blue)</font>
      </figcaption>
      <hr>

      <!--<sup id="upd1" style="font-style: normal">1. Download our demo <a href="https://github.com/peiyunh/tiny">here</a>.
        Last update on 12/22/2016. </sup> -->
    </figure>
  </div>
</div>
  <div class="content">
    <div id="teaser" style="float: right; width:20%; margin: 10px; text-align: center;">
      <a href="https://arxiv.org/pdf/1912.00497.pdf" target="_blank">
        <img src="./files/paper_original.png" width="70%" alt="teaser" style="margin-bottom:10px">
        <strong>Download Paper</strong>
      </a>
    </div>
    <h2>Abstract</h2>
    <p>
    <!-- <b>Put your abstract here. </b> -->
    <br>
      When interacting with highly dynamic environments, scene flow allows autonomous systems to reason about the non-rigid motion of multiple independent objects. This is of particular interest in the field of autonomous driving, in which many cars, people, bicycles, and other objects need to be accurately tracked. Current state of the art methods require annotated scene flow data from autonomous driving scenes to train scene flow networks with supervised learning. As an alternative, we present a method of training scene flow that uses two self-supervised losses, based on nearest neighbors and cycle consistency. These self-supervised losses allow us to train our method on large unlabeled autonomous driving datasets; the resulting method matches current state-of-the-art supervised performance using no real world annotations and exceeds state-of-the-art performance when combining our self-supervised approach with supervised learning on a smaller labeled dataset.
    </p>
  </div>



  <div class="content">
	    <h2>Problem Definition</h2>
	    <div id="overview" float: left">
	    </div>
	    <p>
	      For the task of scene flow estimation, we have a temporal sequence of point clouds recorded from LiDAR: point cloud <b>X</b> as the point cloud captured at time (t) and point cloud <b>Y</b> captured at time (t+1). Each point p(i) = {x(i),f(i)} in point cloud (X) contains the Cartesian position of the point and features such as color, intensity, etc. </p>
<p>
The scene flow between the two point clouds describes the movement of each Cartesian point <b>x(i)</b> in point cloud <b>X</b> to its corresponding position <b>x(i)'</b> in the scene described by point cloud <b>Y</b>.
</p>
    </div>

  
  
  
  <div class="content">
	    <div id="context_human" style="width:42%; margin: 25px; float: right">
	      <a href="./files/losses">
	        <img src="./files/losses" width="100%" height="50%" alt="human_exp">
	      </a>
	    </div>
	    <h2>Losses</h2>
	  	<p>
	  	<b> Nearest Neighbor Loss </b> <br>
	  	For large unlabeled datasets, since we do not have ground truth labels, we cannot compute the supervised loss.  We use the nearest neighbor of our transformed point as an approximation for the true correspondence. For each transformed point in predicted point cloud, we find its nearest neighbor in <b>Y</b> and compute the Euclidean distance with that point.<br> <br>
	  	<b>Cycle Consistency Loss </b> <br>
	  	To avoid degeneracies caused by cycle loss, we incorporate an additional self-supervised loss: cycle-consistency loss. We first estimate the forward flow to get a predicted point cloud. We then compute the scene flow in reverse direction under the cyclic assumption that the prediction of the reverse cycle should be similar to point cloud <b>X</b>. <br> <br>
		<b> Anchored Cycle Consistency Loss </b> <br> 
		In order to avoid unstable results and correct the structural distortions, we compute the anchored reverse flow by taking an average of prediction of foward cycle and point cloud <b>Y</b>. <br> <br>
		<b> Temporal Flip Augmentation </b> <br> 
		Having a dataset of point cloud sequences in only one direction may generate a motion bias. To reduce this bias, we augment the training set by temporally flipping the point clouds i.e. reversing the flow.
	  	</p>
	  	
    </div>
    
    
    
  <div class="content">
    <div id="context" style="width:40%; margin: 25px; margin-top: 0px; float: right">
    </div>
	    <h2>Experiments</h2>
	    <p>
	    <b> Datasets Used </b>: nuScenes and KITTI <br> <br>
	    <b> Evaluation Metric </b>: EPE, Acc1 (0.05), Acc2 (1.0) <br> <br>
	    <b> Self-Supervised training on nuScenes </b> <br>
	    We begin with training our self-supervised model on nuScenes dataset using the combination of Nearest Neighbor Loss and Anchored Cycle loss. Since we wish to use Flownet3D as our scene flow estimation module, we initialize our network with Flownet3D weights pretrained on FlyingThing3D dataset.  <br> <br>
	    <b> Self-Supervised training on nuScenes and KITTI </b> <br>
	      Once the model has been trained on nuScenes, we fine-tune on KITTI in a self-supervised manner. For the comparison with the baseline, we use the Flownet3D model pretrained on FlyingThings3D without any fine-tuning on KITTI. <br> <br>	     
	    <b> Supervised fine-tuning on KITTI </b> <br> 
	    In order to evaluate the performance of our method on the real-world datasets having ground truth flow annotations, we fine tune our model on KITTI. For our method, we pretrain the model on nuScences using our self-supervised loss, and then introduce the KITTI data for supervised fine tuning. For the baseline, we use Flownet3d which is supervised fine tuned over the KITTI dataset. Both models are initialized with Flownet3D weights pretrained on FlyingThings3D.
	    </p>
    </div>
    
    
    
  
  
  

  <div class="content">
    <h2> Qualitative results</h2>
    <div id="results" style="width:100%; text-align: left">
      <a href="./files/self-super-kitti">
        <img src="./files/self-super-kitti" width="97%" alt="examples of different attributes" style="padding-bottom:25px;display:block; margin: 0 auto;">
        <figcaption>Scene flow estimation between point cloud at time 't' <font color="red">(red)</font> and 't+1' <font color="green">(green)</font> from KITTI dataset trained without any labeled lidar data. Our self-supervised method, trained on nuScenes and fine tuned on KITTI using self-supervised loss shown in <font color="blue">(blue)</font> and baseline training method, with no fine tuning, is shown in <font color="magenta">(purple)</font>.  All models are pretrained on FlyingThings3D using a supervised loss. In the absence of any lidar annotations, our method clearly outperforms the baseline method, which over estimate the flow in many regions. (Best viewed in color)</figcaption>
        <br><br>
      </a>
      <a href="./files/nuscenes_comparison">
        <img src="./files/nuscenes_comparison" width="97%" alt="examples of different scales" style="padding-bottom:25px;display:block; margin: 0 auto;">
        <figcaption>Comparison of our self-supervised method vs baseline on unannotated nuScenes dataset. Scene flow is computed  between point cloud at time 't' <font color="red">(red)</font> and 't+1' <font color="green">(green)</font> and the transformed cloud in shown in <font color="blue">(blue)</font>. In our method, the predicted point cloud has a much better overlap with the point cloud of the next timestamp as compared to the baseline. Since nuScenes dataset does not provide any scene flow annotation, the supervised approaches cannot be fined tuned to this environment.</figcaption>
      </a>
      <br><br>
      <a href="./files/supervised_new">
        <img src="./files/supervised_new" width="97%" alt="examples of different scales" style="padding-bottom:25px;display:block; margin: 0 auto;">
        <figcaption>Improved scene flow estimation on annotated lidar data from the KITTI dataset between point clout at time 't' <font color="red">(red)</font> and 't+1' <font color="green">(green)</font>. Our method, which is fine tuned on nuScenes using the self-supervised loss and KITTI using a supervised loss is shown in <font color="blue">(blue)</font>. The baseline method is fine tuned only on KITTI using a supervised loss and is shown in <font color="magenta">(purple)</font>. While in aggregate, both methods well estimate the scene flow, the augmented training method <font color="blue">(blue)</font> is able to more closely match the next frame point cloud <font color="green">(green)</font>. In several of the cropped scenes, the purely supervised method <font color="magenta">(purple)</font> underestimates the flow, staying too close to the initial point cloud <font color="red">(red)</font>. (Best viewed in color)</figcaption>
      </a>
      <br><br>
      <a href="./files/cycle_vs_knn">
        <img src="./files/cycle_vs_knn" width="50%" alt="examples of different scales" style="padding-bottom:25px;display:block; margin: 0 auto;">
        <figcaption>Comparison of levels of supervision on KITTI dataset. The nearest neighbor + anchored cycle loss is used for nuScenes (self-supervised) and KITTI (self-supervised).</figcaption>
      </a>

    </div>
  </div>

  
   <div class="content">
      <div style="float: right; width:70px; margin-top: 0px; margin-bottom: 25px">
	      <a href="">
	       <img src="./files/GitHub-Mark.png" width="100%" alt="github">
	      </a>
      </div>
      <h2>Code</h2>
      <p>
	  Code is available <a href="https://github.com/HimangiM/Just-Go-with-the-Flow-Self-Supervised-Scene-Flow-Estimation">here</a>.
      </p>
    </div>
    
    
    
    
  

  <div class="content">
	    <h2>Acknowledgments</h2>
	    <p> <b> This work was supported by the CMU Argo AI Center for Autonomous Vehicle Research. </b> </p>
    </div>

</body></html> 
