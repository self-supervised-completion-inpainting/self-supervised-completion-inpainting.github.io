
<!-- saved from url=(0046)http://www.cs.cmu.edu/~peiyunh/tiny/index.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <link rel="StyleSheet" href="./files/project.css" type="text/css" media="all">
  <link href="./files/css" rel="stylesheet">
  <title>Self-Supervised Point Cloud Completion via Inpainting</title>
</head>

<body>

  <div class="content content-title" style="text-align: center">
	    <h1>Self-Supervised Point Cloud Completion via Inpainting</h1>
	    <p id="authors">
	      <a href="https://himangim.github.io/" target="_blank">Himangi Mittal</a>
	      <a href="https://www.ri.cmu.edu/ri-people/brian-e-okorn/" target="_blank">Brian Okorn</a>
 	      <a href="https://www.linkedin.com/in/arpit-jangid" target="_blank">Arpit Jangid</a>
	      <a href="https://davheld.github.io/" target="_blank">David Held</a>
                <br>
	      Robotics Institute <br>
	      Carnegie Mellon University
	    </p>
	    <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0443.pdf" target="_blank">[Paper]</a>
	    <a href="https://arxiv.org/abs/2111.10701" target="_blank">[Arxiv Paper]</a>
<!-- 	    <a href="https://github.com/HimangiM/Just-Go-with-the-Flow-Self-Supervised-Scene-Flow-Estimation">[Code]</a>  -->
	    <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0443.html" target="_blank">[Video]</a>
            
    </div>
    
<div class="content">
  <div class="row">
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
     <a href="./files/bmvc_fig1.png">
        <img src="./files/bmvc_fig1.png" width="100%" alt="teaser" style="margin-bottom:10px">
	<!-- <div class="column"> -->
	     <!-- <img src="./files/overview_car.png" width="50%" alt="teaser" style="margin-bottom:10px"> -->
	<!-- </div> -->
	<!-- <div class="column"> -->
		<!-- <img src="./files/twitter.gif" width="50%" alt="teaser_gif" style="margin-bottom:10px"> -->
	<!-- </div> -->
      </a>
      <figcaption> 
	We adopt an inpainting-based approach for self-supervised point cloud completion
to train our network using only partial point clouds. Given a partial point cloud as input, we
randomly remove regions from it and train the network to complete these regions using the
input as the pseudo-ground truth. The loss is only applied to the regions which have points in
the observed input partial point cloud <font color="red">(red)</font>. Since, the network cannot differentiate between
synthetic and natural occlusions, the network predicts a complete point cloud.   
     </figcaption>
      <hr>

      <!--<sup id="upd1" style="font-style: normal">1. Download our demo <a href="https://github.com/peiyunh/tiny">here</a>.
        Last update on 12/22/2016. </sup> -->
    </figure>
  </div>
</div>
  <div class="content">
    <div id="teaser" style="float: right; width:20%; margin: 10px; text-align: center;">
      <a href="https://arxiv.org/abs/2111.10701" target="_blank">
        <img src="./files/paper_first_page.png" width="70%" alt="teaser" style="margin-bottom:10px">
        <strong>Download Paper</strong>
      </a>
    </div>
    <h2>Abstract</h2>
    <p>
    <!-- <b>Put your abstract here. </b> -->
    <br>
    When navigating in urban environments, many of the objects that need to be tracked and avoided are heavily occluded. 
    Planning and tracking using these partial scans can be challenging. The aim of this work is to learn to complete these partial point clouds, 
    giving us a full understanding of the object's geometry using only partial observations. Previous methods achieve this with the help of complete, 
    ground-truth annotations of the target objects, which are available only for simulated datasets. However, such ground truth is unavailable for 
    real-world LiDAR data. In this work, we present a self-supervised point cloud completion algorithm, PointPnCNet, which is trained only on partial 
    scans without assuming access to complete, ground-truth annotations. Our method achieves this via inpainting. We remove a portion of the input data 
    and train the network to complete the missing region. As it is difficult to determine which regions were occluded in the initial cloud and which 
    were synthetically removed, our network learns to complete the full cloud, including the missing regions in the initial partial cloud. We show 
    that our method outperforms previous unsupervised and weakly-supervised methods on both the synthetic dataset, ShapeNet, and real-world 
    LiDAR dataset, Semantic KITTI. 
    </p>
  </div>



  <div class="content">
	    <h2>Problem Definition</h2>
	    <div id="overview" float: "left">
	    </div>
	    <p>
	      The point cloud completion problem can be defined as follows: given an incomplete set of
sparse 3D points X, sampled from a partial view of an underlying dense object geometry G,
the goal is to predict a new set of points Y , which mimics a uniform sampling of G.
    </div>
	
	<div class="content">
	    <h2>Self-Supervised Inpainting</h2>
	    <div id="overview" float: "left">
	    </div>
	    <p>
	      In our self-supervised inpainting-based approach to learn to complete full point clouds using
only partial point clouds, we randomly remove regions of points from a given partial point
cloud and train the network to inpaint these synthetically removed regions. The original
partial point cloud is then used as a pseudo-ground truth to supervise the completion. The network leverages the information of available regions across samples and embeds
each region separately that can generalize across partially occluded samples with different
missing regions. Further, due to the stochastic nature of region removal, the network cannot
easily differentiate between the synthetic and original occlusions of the input partial point
cloud, making the network learn to complete the point cloud. 
    </div>

					   	
	<div class="content">
	    <h2>Network Architecture</h2>
	    <div id="overview" float: "left">
	    </div>
	    <p>
	<figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
     <a href="./files/bmvc_fig1.png">
        <img src="./files/bmvc_fig1.png" width="100%" alt="teaser" style="margin-bottom:10px">
	<!-- <div class="column"> -->
	     <!-- <img src="./files/overview_car.png" width="50%" alt="teaser" style="margin-bottom:10px"> -->
	<!-- </div> -->
	<!-- <div class="column"> -->
		<!-- <img src="./files/twitter.gif" width="50%" alt="teaser_gif" style="margin-bottom:10px"> -->
	<!-- </div> -->
      </a>
      <figcaption> 
	PointPnCNet Architecture: Our method first estimates a canonicalized orientation of a partial point cloud, which has some regions missing due to natural occlusions. We
then randomly drop one or more of the regions to create additional synthetic occlusions. We
compute global features eg and local features P` which we combine into an encoding P. Our
multi-level decoder uses the encoding P to generate a completed point cloud. The global
shape loss and local shape loss are only applied to the regions of the output where points
are present in the original cloud (before synthetic occlusions) which are shown in <font color="red">red</font> in X, Yg, and Y`. The <font color="blue">blue</font> points in Yg and Y` are not present in the original cloud, so we have no
ground truth about their positions; thus they are not penalized in the loss. The final output of
the network is the concatenation of the outputs from Yg and Y`.   
     </figcaption>
		    </figure>
		    <br> <br>
	      <b> Multi-Level Encoder</b> : Our encoder consists of multiple, parallel encoder streams that encode the input partial point
cloud at global and local levels. The global-level encoder operates on the full-scope of
the object while a local-level encoder focuses on a particular region of the object. Since
a local encoder only sees points in a given local region and is invariant to other parts of
the shape which may be missing, local encoders make the network robust to occlusions
by focusing on individual object parts separately. Global encoder further enhances shape
consistency by focusing on regions jointly with each other. 
		  
		    <br> <br>
		    <b>Multi-Level Decoder</b>: Our decoder consists of multiple decoder streams that work in parallel to decode the fused
embedding P. The multi-level output generated by the network captures the details of the object at global
and local levels.
    </div>
  
  
  
  <div class="content">
	    <div id="context_human" style="width:42%; margin: 25px; float: right">
	      <a href="./files/losses">
	        <img src="./files/losses" width="100%" height="50%" alt="human_exp">
	      </a>
	    </div>
	    <h2>Losses</h2>
	  	<p>
	  	<b> Nearest Neighbor Loss </b> <br>
	  	For large unlabeled datasets, since we do not have ground truth labels, we cannot compute the supervised loss.  We use the nearest neighbor of our transformed point as an approximation for the true correspondence. For each transformed point in predicted point cloud, we find its nearest neighbor in <b>Y</b> and compute the Euclidean distance with that point.<br> <br>
	  	<b>Cycle Consistency Loss </b> <br>
	  	To avoid degeneracies caused by cycle loss, we incorporate an additional self-supervised loss: cycle-consistency loss. We first estimate the forward flow to get a predicted point cloud. We then compute the scene flow in reverse direction under the cyclic assumption that the prediction of the reverse cycle should be similar to point cloud <b>X</b>. <br> <br>
		<b> Anchored Cycle Consistency Loss </b> <br> 
		In order to avoid unstable results and correct the structural distortions, we compute the anchored reverse flow by taking an average of prediction of foward cycle and point cloud <b>Y</b>. <br> <br>
		<b> Temporal Flip Augmentation </b> <br> 
		Having a dataset of point cloud sequences in only one direction may generate a motion bias. To reduce this bias, we augment the training set by temporally flipping the point clouds i.e. reversing the flow.
	  	</p>
	  	
    </div>
    
    
    
  <div class="content">
    <div id="context" style="width:40%; margin: 25px; margin-top: 0px; float: right">
    </div>
	    <h2>Experiments</h2>
	    <p>
	    <b> Datasets Used </b>: nuScenes and KITTI <br> <br>
	    <b> Evaluation Metric </b>: EPE, Acc1 (0.05), Acc2 (1.0) <br> <br>
	    <b> Self-Supervised training on nuScenes </b> <br>
	    We begin with training our self-supervised model on nuScenes dataset using the combination of Nearest Neighbor Loss and Anchored Cycle loss. Since we wish to use Flownet3D as our scene flow estimation module, we initialize our network with Flownet3D weights pretrained on FlyingThing3D dataset.  <br> <br>
	    <b> Self-Supervised training on nuScenes and KITTI </b> <br>
	      Once the model has been trained on nuScenes, we fine-tune on KITTI in a self-supervised manner. For the comparison with the baseline, we use the Flownet3D model pretrained on FlyingThings3D without any fine-tuning on KITTI. <br> <br>	     
	    <b> Supervised fine-tuning on KITTI </b> <br> 
	    In order to evaluate the performance of our method on the real-world datasets having ground truth flow annotations, we fine tune our model on KITTI. For our method, we pretrain the model on nuScences using our self-supervised loss, and then introduce the KITTI data for supervised fine tuning. For the baseline, we use Flownet3d which is supervised fine tuned over the KITTI dataset. Both models are initialized with Flownet3D weights pretrained on FlyingThings3D.
	    </p>
    </div>
    
    
    
  
  
  

  <div class="content">
    <h2> Qualitative results</h2>
    <div id="results" style="width:100%; text-align: left">
      <a href="./files/self-super-kitti">
        <img src="./files/self-super-kitti" width="97%" alt="examples of different attributes" style="padding-bottom:25px;display:block; margin: 0 auto;">
        <figcaption>Scene flow estimation between point cloud at time 't' <font color="red">(red)</font> and 't+1' <font color="green">(green)</font> from KITTI dataset trained without any labeled lidar data. Our self-supervised method, trained on nuScenes and fine tuned on KITTI using self-supervised loss shown in <font color="blue">(blue)</font> and baseline training method, with no fine tuning, is shown in <font color="magenta">(purple)</font>.  All models are pretrained on FlyingThings3D using a supervised loss. In the absence of any lidar annotations, our method clearly outperforms the baseline method, which over estimate the flow in many regions. (Best viewed in color)</figcaption>
        <br><br>
      </a>
      <a href="./files/nuscenes_comparison">
        <img src="./files/nuscenes_comparison" width="97%" alt="examples of different scales" style="padding-bottom:25px;display:block; margin: 0 auto;">
        <figcaption>Comparison of our self-supervised method vs baseline on unannotated nuScenes dataset. Scene flow is computed  between point cloud at time 't' <font color="red">(red)</font> and 't+1' <font color="green">(green)</font> and the transformed cloud in shown in <font color="blue">(blue)</font>. In our method, the predicted point cloud has a much better overlap with the point cloud of the next timestamp as compared to the baseline. Since nuScenes dataset does not provide any scene flow annotation, the supervised approaches cannot be fined tuned to this environment.</figcaption>
      </a>
      <br><br>
      <a href="./files/supervised_new">
        <img src="./files/supervised_new" width="97%" alt="examples of different scales" style="padding-bottom:25px;display:block; margin: 0 auto;">
        <figcaption>Improved scene flow estimation on annotated lidar data from the KITTI dataset between point clout at time 't' <font color="red">(red)</font> and 't+1' <font color="green">(green)</font>. Our method, which is fine tuned on nuScenes using the self-supervised loss and KITTI using a supervised loss is shown in <font color="blue">(blue)</font>. The baseline method is fine tuned only on KITTI using a supervised loss and is shown in <font color="magenta">(purple)</font>. While in aggregate, both methods well estimate the scene flow, the augmented training method <font color="blue">(blue)</font> is able to more closely match the next frame point cloud <font color="green">(green)</font>. In several of the cropped scenes, the purely supervised method <font color="magenta">(purple)</font> underestimates the flow, staying too close to the initial point cloud <font color="red">(red)</font>. (Best viewed in color)</figcaption>
      </a>
      <br><br>
      <a href="./files/cycle_vs_knn">
        <img src="./files/cycle_vs_knn" width="50%" alt="examples of different scales" style="padding-bottom:25px;display:block; margin: 0 auto;">
        <figcaption>Comparison of levels of supervision on KITTI dataset. The nearest neighbor + anchored cycle loss is used for nuScenes (self-supervised) and KITTI (self-supervised).</figcaption>
      </a>

    </div>
  </div>

  
<!--    <div class="content">
      <div style="float: right; width:70px; margin-top: 0px; margin-bottom: 25px">
	      <a href="">
	       <img src="./files/GitHub-Mark.png" width="100%" alt="github">
	      </a>
      </div>
      <h2>Code</h2>
      <p>
	  Code is available <a href="https://github.com/HimangiM/Just-Go-with-the-Flow-Self-Supervised-Scene-Flow-Estimation">here</a>.
      </p>
    </div> -->
    
    
    
    
  

  <div class="content">
	    <h2>Acknowledgments</h2>
	    <p> <b> This material is based upon work supported by the National Science Foundation under Grant No. IIS-1849154, 
		      and the CMU Argo AI Center for Autonomous Vehicle Research. </b> </p>
    </div>

</body></html> 
